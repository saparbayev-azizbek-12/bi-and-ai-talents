{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Scrape weather information from an HTML file and process it using Python and BeautifulSoup.\n",
    "\n",
    "<h4>5-Day Weather Forecast</h4>\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Day</th>\n",
    "            <th>Temperature</th>\n",
    "            <th>Condition</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Monday</td>\n",
    "            <td>25°C</td>\n",
    "            <td>Sunny</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Tuesday</td>\n",
    "            <td>22°C</td>\n",
    "            <td>Cloudy</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Wednesday</td>\n",
    "            <td>18°C</td>\n",
    "            <td>Rainy</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Thursday</td>\n",
    "            <td>20°C</td>\n",
    "            <td>Partly Cloudy</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Friday</td>\n",
    "            <td>30°C</td>\n",
    "            <td>Sunny</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "Assume you are given the following HTML structure (you can save it as `weather.html`):\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Weather Forecast</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h4>5-Day Weather Forecast</h4>\n",
    "    <table>\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th>Day</th>\n",
    "                <th>Temperature</th>\n",
    "                <th>Condition</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td>Monday</td>\n",
    "                <td>25°C</td>\n",
    "                <td>Sunny</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Tuesday</td>\n",
    "                <td>22°C</td>\n",
    "                <td>Cloudy</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Wednesday</td>\n",
    "                <td>18°C</td>\n",
    "                <td>Rainy</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Thursday</td>\n",
    "                <td>20°C</td>\n",
    "                <td>Partly Cloudy</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Friday</td>\n",
    "                <td>30°C</td>\n",
    "                <td>Sunny</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "1. **Parse the HTML File**:\n",
    "   - Load the `weather.html` file using BeautifulSoup and extract the weather forecast details.\n",
    "\n",
    "2. **Display Weather Data**:\n",
    "   - Print the **day**, **temperature**, and **condition** for each entry in the forecast.\n",
    "\n",
    "3. **Find Specific Data**:\n",
    "   - Identify and print the day(s) with:\n",
    "     - The highest temperature.\n",
    "     - The \"Sunny\" condition.\n",
    "\n",
    "4. **Calculate Average Temperature**:\n",
    "   - Compute and print the **average temperature** for the week.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day: Monday, Temperature: 25В°C, Condition: Sunny\n",
      "Day: Tuesday, Temperature: 22В°C, Condition: Cloudy\n",
      "Day: Wednesday, Temperature: 18В°C, Condition: Rainy\n",
      "Day: Thursday, Temperature: 20В°C, Condition: Partly Cloudy\n",
      "Day: Friday, Temperature: 30В°C, Condition: Sunny\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the HTML file\n",
    "with open('weather.html', 'r') as file:\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "# Extract weather forecast details\n",
    "forecast = []\n",
    "rows = soup.find('tbody').find_all('tr')\n",
    "for row in rows:\n",
    "    day = row.find_all('td')[0].text\n",
    "    temperature = row.find_all('td')[1].text\n",
    "    condition = row.find_all('td')[2].text\n",
    "    forecast.append({'Day': day, 'Temperature': temperature, 'Condition': condition})\n",
    "\n",
    "# Display weather data\n",
    "for entry in forecast:\n",
    "    print(f\"Day: {entry['Day']}, Temperature: {entry['Temperature']}, Condition: {entry['Condition']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Scrape job listings from the website https://realpython.github.io/fake-jobs and store the data into an SQLite database.\n",
    "\n",
    "1. **Scraping Requirements**:\n",
    "   - Extract the following details for each job listing:\n",
    "     - **Job Title**\n",
    "     - **Company Name**\n",
    "     - **Location**\n",
    "     - **Job Description**\n",
    "     - **Application Link**\n",
    "\n",
    "2. **Data Storage**:\n",
    "   - Store the scraped data into an SQLite database in a table named `jobs`.\n",
    "\n",
    "3. **Incremental Load**:\n",
    "   - Ensure that your script performs **incremental loading**:\n",
    "     - Scrape the webpage and add only **new job listings** to the database.\n",
    "     - Avoid duplicating entries. Use `Job Title`, `Company Name`, and `Location` as unique identifiers for comparison.\n",
    "\n",
    "4. **Update Tracking**:\n",
    "   - Add functionality to detect if an existing job listing has been updated (e.g., description or application link changes) and update the database record accordingly.\n",
    "\n",
    "5. **Filtering and Exporting**:\n",
    "   - Allow filtering job listings by **location** or **company name**.\n",
    "   - Write a function to export filtered results into a CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import csv\n",
    "\n",
    "def scrape_jobs(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    jobs = []\n",
    "    job_elements = soup.find_all('div', class_='card-content')\n",
    "    for job_element in job_elements:\n",
    "        title = job_element.find('h2', class_='title').text.strip()\n",
    "        company = job_element.find('h3', class_='company').text.strip()\n",
    "        location = job_element.find('p', class_='location').text.strip()\n",
    "        application_link = job_element.find('a')['href']\n",
    "        jobs.append((title, company, location, application_link))\n",
    "    return jobs\n",
    "\n",
    "def store_jobs(jobs):\n",
    "    conn = sqlite3.connect('jobs.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS jobs\n",
    "                 (title TEXT, company TEXT, location TEXT, application_link TEXT)''')\n",
    "    for job in jobs:\n",
    "        c.execute('''INSERT OR IGNORE INTO jobs (title, company, location, application_link)\n",
    "                     VALUES (?, ?, ?, ?)''', job)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def filter_jobs(location=None, company=None):\n",
    "    conn = sqlite3.connect('jobs.db')\n",
    "    c = conn.cursor()\n",
    "    query = 'SELECT * FROM jobs WHERE 1=1'\n",
    "    params = []\n",
    "    if location:\n",
    "        query += ' AND location=?'\n",
    "        params.append(location)\n",
    "    if company:\n",
    "        query += ' AND company=?'\n",
    "        params.append(company)\n",
    "    c.execute(query, params)\n",
    "    jobs = c.fetchall()\n",
    "    conn.close()\n",
    "    return jobs\n",
    "\n",
    "def export_jobs_to_csv(jobs, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        job_writer = csv.writer(csvfile)\n",
    "        job_writer.writerow(['Title', 'Company', 'Location', 'Application Link'])\n",
    "        job_writer.writerows(jobs)\n",
    "\n",
    "jobs = scrape_jobs(\"https://realpython.github.io/fake-jobs\")\n",
    "store_jobs(jobs)\n",
    "\n",
    "filtered_jobs = filter_jobs(location='Remote')\n",
    "export_jobs_to_csv(filtered_jobs, 'filtered_jobs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "You are tasked with scraping laptop data from the \"Laptops\" section of the [Demoblaze website](https://www.demoblaze.com/) and storing the extracted information in JSON format.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Navigate to the Website:**\n",
    "   - Visit the [Demoblaze homepage](https://www.demoblaze.com/).\n",
    "   - Click on the **Laptops** section to view the list of available laptops.\n",
    "\n",
    "2. **Navigate to the Next Page:**\n",
    "   - After reaching the Laptops section, locate and click the **Next** button to navigate to the next page of laptop listings.\n",
    "\n",
    "3. **Data to Scrape:**\n",
    "   For each laptop on the page, scrape the following details:\n",
    "   - **Laptop Name**\n",
    "   - **Price**\n",
    "   - **Description**\n",
    "\n",
    "4. **Data Storage:**\n",
    "   - Save the extracted information in a structured **JSON format** with fields like:\n",
    "     ```json\n",
    "     [\n",
    "       {\n",
    "         \"name\": \"Laptop Name\",\n",
    "         \"price\": \"Laptop Price\",\n",
    "         \"description\": \"Laptop Description\"\n",
    "       },\n",
    "       ...\n",
    "     ]\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laptop data has been scraped and saved to laptops.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def scrape_laptops(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    laptops = []\n",
    "    laptop_elements = soup.find_all('div', class_='card-block')\n",
    "    for laptop_element in laptop_elements:\n",
    "        name = laptop_element.find('h4', class_='card-title').text.strip()\n",
    "        price = laptop_element.find('h5').text.strip()\n",
    "        description = laptop_element.find('p', class_='card-text').text.strip()\n",
    "        laptops.append({'name': name, 'price': price, 'description': description})\n",
    "    return laptops\n",
    "\n",
    "def scrape_all_laptops(base_url):\n",
    "    all_laptops = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        url = f\"{base_url}#page={page}\"\n",
    "        laptops = scrape_laptops(url)\n",
    "        if not laptops:\n",
    "            break\n",
    "        all_laptops.extend(laptops)\n",
    "        page += 1\n",
    "    return all_laptops\n",
    "\n",
    "base_url = \"https://www.demoblaze.com/index.html\"\n",
    "\n",
    "all_laptops = scrape_all_laptops(base_url)\n",
    "\n",
    "with open('laptops.json', 'w') as json_file:\n",
    "    json.dump(all_laptops, json_file, indent=4)\n",
    "\n",
    "print(\"Laptop data has been scraped and saved to laptops.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
